# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AOwYHMgQ2zzlnVMz-4A40UJvFc2LdyLO
"""

!pip install transformers sentence-transformers faiss-cpu pymupdf streamlit pyngrok --upgrade

# Import necessary libraries
import fitz  # PyMuPDF
from transformers import pipeline
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
import streamlit as st
from pyngrok import ngrok

# Function to extract text from a PDF using PyMuPDF
def extract_text_from_pdf(pdf_path):
    doc = fitz.open(pdf_path)
    text = ""
    for page in doc:
        text += page.get_text("text")  # Extract plain text from each page
    return text

# Initialize the Granite model for text generation
pipe = pipeline("text-generation", model="ibm-granite/granite-3.2-2b-instruct")

# Function to create embeddings for PDF text chunks using SentenceTransformer
def create_embeddings(pdf_text):
    model = SentenceTransformer('all-MiniLM-L6-v2')
    return model.encode(pdf_text, convert_to_tensor=True)

# Function to create a FAISS index for the embeddings
def create_faiss_index(embeddings):
    faiss_index = faiss.IndexFlatL2(embeddings.shape[1])
    faiss_index.add(np.array(embeddings))
    return faiss_index

# Function to get the most relevant chunk for a query using FAISS
def get_relevant_chunk(query, pdf_text, faiss_index):
    query_embedding = model.encode([query])
    _, indices = faiss_index.search(query_embedding, k=1)
    return pdf_text[indices[0][0]]  # Return the most relevant chunk of text

# Function to generate an answer using the Granite model
def generate_answer(query, relevant_chunk):
    response = pipe([{"role": "user", "content": query + " " + relevant_chunk}])
    return response[0]['generated_text']

# Streamlit interface setup
def run_streamlit_app():
    st.title("StudyMate: AI-Powered Q&A System")

    # File Upload
    uploaded_file = st.file_uploader("Upload your PDF", type="pdf")
    if uploaded_file is not None:
        # Extract text from the uploaded PDF
        text = extract_text_from_pdf(uploaded_file)
        pdf_text = text.split('\n')  # Split text into chunks (could be pages, paragraphs, etc.)

        # Create embeddings and FAISS index
        embeddings = create_embeddings(pdf_text)
        faiss_index = create_faiss_index(embeddings)

        st.text_area("Extracted Text", text[:2000])  # Show first 2000 characters of extracted text

    # User Query Input
    question = st.text_input("Ask a question")
    if question:
        # Find the most relevant chunk
        relevant_chunk = get_relevant_chunk(question, pdf_text, faiss_index)

        # Generate the answer using the Granite model
        answer = generate_answer(question, relevant_chunk)
        st.write(answer)

# Expose the Streamlit app using ngrok
public_url = ngrok.connect(port='8501')
print(f"Streamlit app is available at {public_url}")

# Run the Streamlit app
run_streamlit_app()

"""To use ngrok, you need an authtoken. If you don't already have one, get it from your ngrok dashboard at [https://dashboard.ngrok.com/get-started/your-authtoken](https://dashboard.ngrok.com/get-started/your-authtoken). Add it to the Colab secrets manager under the "ðŸ”‘" tab on the left panel with the name `NGROK_AUTH_TOKEN`. Then, run the following code to authenticate ngrok:"""

from pyngrok import ngrok
from google.colab import userdata

# Get the authtoken from Colab secrets
authtoken = userdata.get('NGROK_AUTH_TOKEN')

# Authenticate ngrok
ngrok.set_auth_token(authtoken)



